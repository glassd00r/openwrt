From bee84a4dd84dcc80563813c0efd13e34398b8f15 Mon Sep 17 00:00:00 2001
Message-ID: <bee84a4dd84dcc80563813c0efd13e34398b8f15.1769700282.git.lorenzo@kernel.org>
In-Reply-To: <48d352f87a6acd3ca35bef9ee6d8f42e0b222924.1769700282.git.lorenzo@kernel.org>
References: <48d352f87a6acd3ca35bef9ee6d8f42e0b222924.1769700282.git.lorenzo@kernel.org>
From: Lorenzo Bianconi <lorenzo@kernel.org>
Date: Tue, 10 Jun 2025 09:33:45 +0200
Subject: [nf-next 4/4] net: airoha: Add TCP LRO support

EN7581 SoC supports TCP hw Large Receive Offload (LRO) for 8 hw queues.
Introduce TCP LRO support to airoha_eth driver for RX queues 0-7.
When hw TCP LRO is enable, increase page_pool order to 2 for RX queues
0-7.

Signed-off-by: Lorenzo Bianconi <lorenzo@kernel.org>
---
 drivers/net/ethernet/airoha/airoha_eth.c  | 261 ++++++++++++++++++++--
 drivers/net/ethernet/airoha/airoha_eth.h  |  11 +
 drivers/net/ethernet/airoha/airoha_regs.h |  22 +-
 3 files changed, 280 insertions(+), 14 deletions(-)

--- a/drivers/net/ethernet/airoha/airoha_eth.c
+++ b/drivers/net/ethernet/airoha/airoha_eth.c
@@ -13,6 +13,7 @@
 #include <net/dst_metadata.h>
 #include <net/page_pool/helpers.h>
 #include <net/pkt_cls.h>
+#include <net/tcp.h>
 #include <uapi/linux/ppp_defs.h>
 
 #include "airoha_regs.h"
@@ -454,6 +455,50 @@ static void airoha_fe_crsn_qsel_init(str
 				 CDM_CRSN_QSEL_Q1));
 }
 
+static void airoha_fe_lro_init_rx_queue(struct airoha_eth *eth,
+					int lro_queue_index, int qid,
+					int nbuf, int buf_size)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(eth->qdma); i++) {
+		int id = i + 1;
+
+		airoha_fe_rmw(eth, REG_CDM_LRO_LIMIT(id),
+			      CDM_LRO_AGG_NUM_MASK | CDM_LRO_AGG_SIZE_MASK,
+			      FIELD_PREP(CDM_LRO_AGG_NUM_MASK, nbuf) |
+			      FIELD_PREP(CDM_LRO_AGG_SIZE_MASK, buf_size));
+		airoha_fe_rmw(eth, REG_CDM_LRO_AGE_TIME(id),
+			      CDM_LRO_AGE_TIME_MASK | CDM_LRO_AGG_TIME_MASK,
+			      FIELD_PREP(CDM_LRO_AGE_TIME_MASK,
+					 AIROHA_RXQ_LRO_MAX_AGE_TIME) |
+			      FIELD_PREP(CDM_LRO_AGG_TIME_MASK,
+					 AIROHA_RXQ_LRO_MAX_AGG_TIME));
+		airoha_fe_rmw(eth, REG_CDM_LRO_RXQ(id, lro_queue_index),
+			      LRO_RXQ_MASK(lro_queue_index),
+			      qid << __ffs(LRO_RXQ_MASK(lro_queue_index)));
+		airoha_fe_set(eth, REG_CDM_LRO_EN(id), BIT(lro_queue_index));
+	}
+}
+
+static void airoha_fe_lro_disable(struct airoha_eth *eth)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(eth->qdma); i++) {
+		int j, id = i + 1;
+
+		airoha_fe_clear(eth, REG_CDM_LRO_LIMIT(id),
+				CDM_LRO_AGG_NUM_MASK | CDM_LRO_AGG_SIZE_MASK);
+		airoha_fe_clear(eth, REG_CDM_LRO_AGE_TIME(id),
+				CDM_LRO_AGE_TIME_MASK | CDM_LRO_AGG_TIME_MASK);
+		airoha_fe_clear(eth, REG_CDM_LRO_EN(id), LRO_RXQ_EN_MASK);
+		for (j = 0; j < AIROHA_MAX_NUM_LRO_QUEUES; j++)
+			airoha_fe_clear(eth, REG_CDM_LRO_RXQ(id, j),
+					LRO_RXQ_MASK(j));
+	}
+}
+
 static int airoha_fe_init(struct airoha_eth *eth)
 {
 	airoha_fe_maccr_init(eth);
@@ -613,9 +658,87 @@ airoha_qdma_get_gdm_dev(struct airoha_et
 	return port->devs[d] ? port->devs[d] : ERR_PTR(-ENODEV);
 }
 
+static bool airoha_qdma_is_lro_rx_queue(struct airoha_queue *q,
+					struct airoha_qdma *qdma)
+{
+	int qid = q - &qdma->q_rx[0];
+
+	/* EN7581 SoC supports at most 8 LRO rx queues */
+	BUILD_BUG_ON(hweight32(AIROHA_RXQ_LRO_EN_MASK) >
+		     AIROHA_MAX_NUM_LRO_QUEUES);
+
+	return !!(AIROHA_RXQ_LRO_EN_MASK & BIT(qid));
+}
+
+static int airoha_qdma_lro_rx_process(struct airoha_queue *q,
+				      struct airoha_qdma_desc *desc)
+{
+	u32 msg1 = le32_to_cpu(desc->msg1), msg2 = le32_to_cpu(desc->msg2);
+	u32 th_off, tcp_ack_seq, msg3 = le32_to_cpu(desc->msg3);
+	bool ipv4 = FIELD_GET(QDMA_ETH_RXMSG_IP4_MASK, msg1);
+	bool ipv6 = FIELD_GET(QDMA_ETH_RXMSG_IP6_MASK, msg1);
+	struct sk_buff *skb = q->skb;
+	u16 tcp_win, l2_len;
+	struct tcphdr *th;
+
+	if (FIELD_GET(QDMA_ETH_RXMSG_AGG_COUNT_MASK, msg2) <= 1)
+		return 0;
+
+	if (!ipv4 && !ipv6)
+		return -EOPNOTSUPP;
+
+	l2_len = FIELD_GET(QDMA_ETH_RXMSG_L2_LEN_MASK, msg2);
+	if (ipv4) {
+		u16 agg_len = FIELD_GET(QDMA_ETH_RXMSG_AGG_LEN_MASK, msg3);
+		struct iphdr *iph = (struct iphdr *)(skb->data + l2_len);
+
+		if (iph->protocol != IPPROTO_TCP)
+			return -EOPNOTSUPP;
+
+		iph->tot_len = cpu_to_be16(agg_len);
+		iph->check = 0;
+		iph->check = ip_fast_csum((void *)iph, iph->ihl);
+		th_off = l2_len + (iph->ihl << 2);
+	} else {
+		struct ipv6hdr *ip6h = (struct ipv6hdr *)(skb->data + l2_len);
+		u32 len, desc_ctrl = le32_to_cpu(desc->ctrl);
+
+		if (ip6h->nexthdr != NEXTHDR_TCP)
+			return -EOPNOTSUPP;
+
+		len = FIELD_GET(QDMA_DESC_LEN_MASK, desc_ctrl);
+		ip6h->payload_len = cpu_to_be16(len - l2_len - sizeof(*ip6h));
+		th_off = l2_len + sizeof(*ip6h);
+	}
+
+	tcp_win = FIELD_GET(QDMA_ETH_RXMSG_TCP_WIN_MASK, msg3);
+	tcp_ack_seq = le32_to_cpu(desc->data);
+
+	th = (struct tcphdr *)(skb->data + th_off);
+	th->ack_seq = cpu_to_be32(tcp_ack_seq);
+	th->window = cpu_to_be16(tcp_win);
+
+	/* check tcp timestamp option */
+	if (th->doff == sizeof(*th) + TCPOLEN_TSTAMP_ALIGNED) {
+		__be32 *topt = (__be32 *)(th + 1);
+
+		if (*topt == cpu_to_be32((TCPOPT_NOP << 24) |
+					 (TCPOPT_NOP << 16) |
+					 (TCPOPT_TIMESTAMP << 8) |
+					 TCPOLEN_TIMESTAMP)) {
+			u32 tcp_ts_reply = le32_to_cpu(desc->tcp_ts_reply);
+
+			put_unaligned_be32(tcp_ts_reply, topt + 2);
+		}
+	}
+
+	return 0;
+}
+
 static int airoha_qdma_rx_process(struct airoha_queue *q, int budget)
 {
 	enum dma_data_direction dir = page_pool_get_dma_dir(q->page_pool);
+	bool lro_queue = airoha_qdma_is_lro_rx_queue(q, q->qdma);
 	struct airoha_qdma *qdma = q->qdma;
 	struct airoha_eth *eth = qdma->eth;
 	int qid = q - &qdma->q_rx[0];
@@ -657,9 +780,14 @@ static int airoha_qdma_rx_process(struct
 			__skb_put(q->skb, len);
 			skb_mark_for_recycle(q->skb);
 			q->skb->dev = dev->dev;
-			q->skb->protocol = eth_type_trans(q->skb, dev->dev);
 			q->skb->ip_summed = CHECKSUM_UNNECESSARY;
 			skb_record_rx_queue(q->skb, qid);
+
+			if (lro_queue && (dev->dev->features & NETIF_F_LRO) &&
+			    airoha_qdma_lro_rx_process(q, desc) < 0)
+				goto free_frag;
+
+			q->skb->protocol = eth_type_trans(q->skb, dev->dev);
 		} else { /* scattered frame */
 			struct skb_shared_info *shinfo = skb_shinfo(q->skb);
 			int nr_frags = shinfo->nr_frags;
@@ -747,14 +875,16 @@ static int airoha_qdma_rx_napi_poll(stru
 	return done;
 }
 
-static int airoha_qdma_init_rx_pp(struct airoha_queue *q)
+static int airoha_qdma_init_rx_pp(struct airoha_queue *q,
+				  bool lro_queue)
 {
+	int pp_order = lro_queue ? AIROHA_LRO_PAGE_ORDER : 0;
 	const struct page_pool_params pp_params = {
-		.order = 0,
-		.pool_size = 256,
+		.order = pp_order,
+		.pool_size = 256 >> pp_order,
 		.flags = PP_FLAG_DMA_MAP | PP_FLAG_DMA_SYNC_DEV,
 		.dma_dir = DMA_FROM_DEVICE,
-		.max_len = PAGE_SIZE,
+		.max_len = PAGE_SIZE << pp_order,
 		.nid = NUMA_NO_NODE,
 		.dev = q->qdma->eth->dev,
 		.napi = &q->napi,
@@ -768,7 +898,7 @@ static int airoha_qdma_init_rx_pp(struct
 		return err;
 	}
 
-	q->buf_size = PAGE_SIZE / 2;
+	q->buf_size = pp_params.max_len / (2 * (1 + lro_queue));
 
 	return 0;
 }
@@ -800,7 +930,8 @@ static int airoha_qdma_init_rx_queue(str
 	q->ndesc = ndesc;
 	q->qdma = qdma;
 
-	err = airoha_qdma_init_rx_pp(q);
+	/* Disable LRO by default */
+	err = airoha_qdma_init_rx_pp(q, false);
 	if (err)
 		return err;
 
@@ -1063,6 +1194,17 @@ static int airoha_qdma_tx_irq_init(struc
 	return 0;
 }
 
+static void airoha_qdma_reset_tx(struct airoha_qdma *qdma)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(qdma->q_tx_irq); i++)
+		airoha_qdma_setup_tx_irq(&qdma->q_tx_irq[i]);
+
+	for (i = 0; i < ARRAY_SIZE(qdma->q_tx); i++)
+		airoha_qdma_setup_tx_queue(&qdma->q_tx[i]);
+}
+
 static int airoha_qdma_init_tx(struct airoha_qdma *qdma)
 {
 	int i, err;
@@ -2063,6 +2205,130 @@ int airoha_get_fe_port(struct airoha_gdm
 					   : port->id;
 }
 
+static int airoha_dev_set_features(struct net_device *netdev,
+				   netdev_features_t features)
+{
+	netdev_features_t diff = netdev->features ^ features;
+	struct airoha_gdm_dev *dev = netdev_priv(netdev);
+	struct airoha_gdm_port *port = dev->port;
+	struct airoha_qdma *qdma = port->qdma;
+	struct airoha_eth *eth = qdma->eth;
+	int i, err, lro_queue_index = 0;
+	u32 val, pse_port, gdm_rxchn_en;
+	struct airoha_queue *q;
+
+	if (!(diff & NETIF_F_LRO))
+		return 0;
+
+	/* disable Tx */
+	netif_tx_disable(netdev);
+	/* wait for Tx to complete */
+	err = read_poll_timeout(airoha_qdma_rr, val,
+				!(val & GLOBAL_CFG_TX_DMA_BUSY_MASK),
+				USEC_PER_MSEC, 10 * USEC_PER_MSEC, false,
+				qdma, REG_QDMA_GLOBAL_CFG);
+	if (err)
+		return err;
+
+	/* disable Tx DMA */
+	airoha_qdma_clear(qdma, REG_QDMA_GLOBAL_CFG,
+			  GLOBAL_CFG_TX_DMA_EN_MASK);
+
+	/* stop Tx napi */
+	for (i = 0; i < ARRAY_SIZE(qdma->q_tx_irq); i++)
+		napi_disable(&qdma->q_tx_irq[i].napi);
+
+	/* reset Tx queues */
+	for (i = 0; i < ARRAY_SIZE(qdma->q_tx); i++) {
+		netdev_tx_reset_subqueue(netdev, i);
+		if (!qdma->q_tx[i].ndesc)
+			continue;
+
+		airoha_qdma_cleanup_tx_queue(&qdma->q_tx[i]);
+	}
+
+	/* disable FE GDMA channel */
+	gdm_rxchn_en = airoha_fe_rr(eth, REG_GDM_RXCHN_EN(port->id));
+	airoha_fe_wr(eth, REG_GDM_RXCHN_EN(port->id), 0x0);
+	/* disable PPE forward to QDMA */
+	pse_port = FIELD_GET(GDM_UCFQ_MASK,
+			     airoha_fe_rr(eth, REG_GDM_FWD_CFG(port->id)));
+	airoha_set_gdm_port_fwd_cfg(eth, REG_GDM_FWD_CFG(port->id),
+				    FE_PSE_PORT_DROP);
+	/* wait for Rx to complete */
+	err = read_poll_timeout(airoha_qdma_rr, val,
+				!(val & GLOBAL_CFG_RX_DMA_BUSY_MASK),
+				USEC_PER_MSEC, 10 * USEC_PER_MSEC, false,
+				qdma, REG_QDMA_GLOBAL_CFG);
+	if (err)
+		return err;
+
+	/* disable Rx DMA */
+	airoha_qdma_clear(qdma, REG_QDMA_GLOBAL_CFG,
+			  GLOBAL_CFG_RX_DMA_EN_MASK);
+
+	/* stop rx napi */
+	for (i = 0; i < ARRAY_SIZE(qdma->q_rx); i++) {
+		q = &qdma->q_rx[i];
+		if (!q->ndesc)
+			continue;
+
+		if (!airoha_qdma_is_lro_rx_queue(q, qdma))
+			continue;
+
+		napi_disable(&q->napi);
+		airoha_qdma_rx_queue_deinit(q);
+	}
+
+	/* reset LRO configuration */
+	airoha_fe_lro_disable(eth);
+	/* reconfigure tx queues */
+	airoha_qdma_reset_tx(qdma);
+
+	/* reconfigure rx queues */
+	for (i = 0; i < ARRAY_SIZE(qdma->q_rx); i++) {
+		int err;
+
+		q = &qdma->q_rx[i];
+		if (!q->ndesc)
+			continue;
+
+		if (!airoha_qdma_is_lro_rx_queue(q, qdma))
+			continue;
+
+		err = airoha_qdma_init_rx_pp(q, features & NETIF_F_LRO);
+		if (err)
+			return err;
+
+		airoha_qdma_setup_rx_queue(q);
+		airoha_qdma_fill_rx_queue(q);
+		airoha_fe_lro_init_rx_queue(eth, lro_queue_index, i,
+					    q->page_pool->p.pool_size,
+					    q->buf_size);
+		lro_queue_index++;
+		napi_enable(&q->napi);
+	}
+
+	err = airoha_qdma_init_hfwd_queues(qdma);
+	if (err)
+		return err;
+
+	/* restart Tx */
+	for (i = 0; i < ARRAY_SIZE(qdma->q_tx_irq); i++)
+		napi_enable(&qdma->q_tx_irq[i].napi);
+	netif_tx_start_all_queues(netdev);
+
+	/* restore FE GDMA channel */
+	airoha_fe_wr(eth, REG_GDM_RXCHN_EN(port->id), gdm_rxchn_en);
+	/* restore PPE forward to QDMA */
+	airoha_set_gdm_port_fwd_cfg(eth, REG_GDM_FWD_CFG(port->id), pse_port);
+
+	airoha_qdma_set(qdma, REG_QDMA_GLOBAL_CFG,
+			GLOBAL_CFG_TX_DMA_EN_MASK | GLOBAL_CFG_RX_DMA_EN_MASK);
+
+	return 0;
+}
+
 static netdev_tx_t airoha_dev_xmit(struct sk_buff *skb,
 				   struct net_device *netdev)
 {
@@ -3114,6 +3380,7 @@ static const struct net_device_ops airoh
 	.ndo_stop		= airoha_dev_stop,
 	.ndo_change_mtu		= airoha_dev_change_mtu,
 	.ndo_select_queue	= airoha_dev_select_queue,
+	.ndo_set_features	= airoha_dev_set_features,
 	.ndo_start_xmit		= airoha_dev_xmit,
 	.ndo_get_stats64        = airoha_dev_get_stats64,
 	.ndo_set_mac_address	= airoha_dev_set_macaddr,
@@ -3318,11 +3585,9 @@ static int airoha_alloc_gdm_device(struc
 	netdev->ethtool_ops = &airoha_ethtool_ops;
 	netdev->max_mtu = AIROHA_MAX_MTU;
 	netdev->watchdog_timeo = 5 * HZ;
-	netdev->hw_features = NETIF_F_IP_CSUM | NETIF_F_RXCSUM | NETIF_F_TSO6 |
-			      NETIF_F_IPV6_CSUM | NETIF_F_SG | NETIF_F_TSO |
-			      NETIF_F_HW_TC;
-	netdev->features |= netdev->hw_features;
-	netdev->vlan_features = netdev->hw_features;
+	netdev->hw_features = AIROHA_HW_FEATURES | NETIF_F_LRO;
+	netdev->features |= AIROHA_HW_FEATURES;
+	netdev->vlan_features = AIROHA_HW_FEATURES;
 	netdev->dev.of_node = np;
 	netdev->irq = port->qdma->irq_banks[0].irq;
 	SET_NETDEV_DEV(netdev, eth->dev);
--- a/drivers/net/ethernet/airoha/airoha_eth.h
+++ b/drivers/net/ethernet/airoha/airoha_eth.h
@@ -42,6 +42,17 @@
 	 (_n) == 15 ? 128 :		\
 	 (_n) ==  0 ? 1024 : 16)
 
+#define AIROHA_LRO_PAGE_ORDER		2
+#define AIROHA_MAX_NUM_LRO_QUEUES	8
+#define AIROHA_RXQ_LRO_EN_MASK		0xff
+#define AIROHA_RXQ_LRO_MAX_AGG_TIME	100
+#define AIROHA_RXQ_LRO_MAX_AGE_TIME	2000 /* 1ms */
+
+#define AIROHA_HW_FEATURES			\
+	(NETIF_F_IP_CSUM | NETIF_F_RXCSUM |	\
+	 NETIF_F_TSO6 | NETIF_F_IPV6_CSUM |	\
+	 NETIF_F_SG | NETIF_F_TSO | NETIF_F_HW_TC)
+
 #define PSE_RSV_PAGES			128
 #define PSE_QUEUE_RSV_PAGES		64
 
--- a/drivers/net/ethernet/airoha/airoha_regs.h
+++ b/drivers/net/ethernet/airoha/airoha_regs.h
@@ -122,6 +122,20 @@
 #define CDM_CRSN_QSEL_REASON_MASK(_n)	\
 	GENMASK(4 + (((_n) % 4) << 3),	(((_n) % 4) << 3))
 
+#define REG_CDM_LRO_RXQ(_n, _m)		(CDM_BASE(_n) + 0x78 + ((_m) & 0x4))
+#define LRO_RXQ_MASK(_n)		GENMASK(4 + (((_n) & 0x3) << 3), ((_n) & 0x3) << 3)
+
+#define REG_CDM_LRO_EN(_n)		(CDM_BASE(_n) + 0x80)
+#define LRO_RXQ_EN_MASK			GENMASK(7, 0)
+
+#define REG_CDM_LRO_LIMIT(_n)		(CDM_BASE(_n) + 0x84)
+#define CDM_LRO_AGG_NUM_MASK		GENMASK(23, 16)
+#define CDM_LRO_AGG_SIZE_MASK		GENMASK(15, 0)
+
+#define REG_CDM_LRO_AGE_TIME(_n)	(CDM_BASE(_n) + 0x88)
+#define CDM_LRO_AGE_TIME_MASK		GENMASK(31, 16)
+#define CDM_LRO_AGG_TIME_MASK		GENMASK(15, 0)
+
 #define REG_GDM_FWD_CFG(_n)		GDM_BASE(_n)
 #define GDM_PAD_EN_MASK			BIT(28)
 #define GDM_DROP_CRC_ERR_MASK		BIT(23)
@@ -896,9 +910,15 @@
 #define QDMA_ETH_RXMSG_SPORT_MASK	GENMASK(25, 21)
 #define QDMA_ETH_RXMSG_CRSN_MASK	GENMASK(20, 16)
 #define QDMA_ETH_RXMSG_PPE_ENTRY_MASK	GENMASK(15, 0)
+/* RX MSG2 */
+#define QDMA_ETH_RXMSG_AGG_COUNT_MASK	GENMASK(31, 24)
+#define QDMA_ETH_RXMSG_L2_LEN_MASK	GENMASK(6, 0)
+/* RX MSG3 */
+#define QDMA_ETH_RXMSG_AGG_LEN_MASK	GENMASK(31, 16)
+#define QDMA_ETH_RXMSG_TCP_WIN_MASK	GENMASK(15, 0)
 
 struct airoha_qdma_desc {
-	__le32 rsv;
+	__le32 tcp_ts_reply;
 	__le32 ctrl;
 	__le32 addr;
 	__le32 data;
